<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                    tex2jax: {
                    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
                    inlineMath: [['$','$']]
                    }
                });
    </script>
</head>



## 2.3 如何控制风速火候，模型结构
近年来基于神经网络的方法大量应用于语音识别任务，唤醒任务作为语音识别任务的一个分支，借鉴了很多模型结构，同时基于本身资源开销小，任务相对简单，基于远场语音等特点，相应做了优化改进。

### 2.3.1 DNN
google比较早地在2014年提出用深度神经网络deep neural networks的方法来实现语音唤醒，称之为Deep KWS。如下图所示，唤醒分为三个步骤，
  ![deep_kws_struct.png](/assets/nn-struct/deep_kws_struct.png)
首先对输入语音做特征提取，然后经过DNN网络得到一个三分类的后验概率，三分类分别对应关键字Okey，Google和其他，最后经过后处理得到置信度得分用于唤醒判决。

- 特征提取

出于减少计算量的考量，这里包含了一个VAD检测机制，用一个13维的PLP特征和他们的一阶差分二阶差分通过一个高斯混合模型 GMM得到每帧人声和非人声的后验概率，再通过平滑和阈值判断决定出人声范围。在人声范围内，基于25ms窗长和10ms窗移得到40维FBank特征。每次输入给模型的特征都在当前帧基础上前后拼接了一部分状态，权衡计算量，时延和精度，Deep KWS在实现中向前拼接了10帧，向后拼接了30帧。

- 神经网络

网络结构部分用的是标准的全连接网络，包含$k$层隐层，每层隐层包含$n$个节点和RELU作为激活函数，最后一层通过softmax得到每个标签的后验概率。Deep KWS的标签只用来表示完整词，即完整包含整个激活词作为一个标签，这些标签来自于一个50M LVCSR大模型的强制对齐。相比于非完整词sub-word标签，完整词标签的好处是，
	- 减少最后一层的网络参数
	- 使得后处理更简单
	- 更好性能
训练中采用交叉熵作为损失函数，同时提到，可以复用其他语音识别网络结构来初始化隐层，实现迁移学习，避免训练陷入局部最小值从而提高模型性能。

- 后处理判决

得到基于帧的标签后验概率之后，后处理部分将后验概率经过平滑处理，得到唤醒置信度得分。平滑过程是为了消除原始后验概率噪声，假设$$p'_{ij}$$是原始后验概率$$p'_{ij}$$平滑之后的结果，平滑窗口为$$w_{smooth}$$，平滑公式如下

$$
p'_{ij}=\frac{1}{j-h_{smooth}+1}\sum_{k=h_{smooth}}^j p_{ik}
$$

其中$$h_{smooth}=max\{1,j-w_{smooth}+1\}$$，是平滑窗$$w_{smooth}$$内的最早帧号索引。
然后基于平滑之后的后验概率$$p'_{ij}$$计算得到置信度，在一个滑动窗$$w_{max}$$第$$j$$帧的置信度

$$
confidence=\sqrt[n-1]{\prod_{i=1}^{n-1}\max \limits_{h_{max}<k<j}p'_{ik}}
$$

其中$$h_{max}=max\{1,j-w_{max}+1\}$$，是平滑窗$$w_{max}$$内的最早帧号索引。
计算得到的置信度和预定义的阈值比较，做出唤醒判决。Deep KWS中使用的$$w_{smooth}=30$$和$$w_{max}=100$$，能得到相对较好的性能。

### 2.3.2 CNN
在过去几年里，卷积神经网络 CNN越来越多的应用在声学模型上，CNN相比于DNN的优势在于
- DNN不关心频谱结构，输入特征做任何拓扑变形也不会影响最终性能，然而我们认为频谱在时频域都有高度相关性，CNN在抓取空间信息方面更有优势

- CNN通过对不同时频区域内的隐层节点输出取平均的方式，比DNN用更少的参数量，克服不同的说话风格带来的共振峰偏移问题

google在2015年提出基于CNN的KWS模型，典型的卷积网络结构包含一层卷积层加一层max池化pooling层。
  ![cnn_kws_struct.png](/assets/nn-struct/cnn_kws_struct.png)
这里输入特征$$V$$的时域维度是$$t$$，频域维度是$$f$$，经过一个$$n$$个$$(m\times r)$$的卷积核，同时卷积步长是$$(s,v)$$，输出$$n$$个$$(\frac{(t-m+1)}{s}\times \frac{(f-r+1)}{v})$$的feature map。卷积之后接一个max池化层提高稳定性，这里pooling降采样系数是$$(p\times q)$$，那么最终输出feature map是$$(\frac{(t-m+1)}{s\dot p}\times \frac{(f-r+1)}{v\dot q})$$。
这里给出一个250K参数量的模型，包含2层卷积结构，一层线性低阶，一层全连接层，具体参数如下

| type    | m    | r    | n    | p    | q    | Par.   | Mul.  |
| ------- | ---- | ---- | ---- | ---- | ---- | ------ | ----- |
| conv    | 20   | 8    | 64   | 1    | 3    | 10.2K  | 4.4M  |
| conv    | 10   | 4    | 64   | 1    | 1    | 164.8K | 5.2M  |
| lin     |      |      | 32   |      |      | 65.5K  | 65,5K |
| dnn     |      |      | 128  |      |      | 4.1K   | 4.1K  |
| softmax |      |      | 4    |      |      | 0.5K   | 0.5K  |
| Total   |      |      |      |      |      | 244.2K | 9.7M  |

在此基础上一系列的参数实验得出一些经验
- 保证同等误激活和参数量前提下，CNN模型比DNN模型性能提高40%以上。
- 随着pooling长度$$p=1$$到$$p=2$$，安静和噪音下唤醒性得到提升，但是$$p=3$$之后没有明显改善。
- 同等计算量条件下，卷积核滑动尽量重叠。比如对比卷积核$$(32\times 8\times 186)$$，步长$$(1,4)$$，和卷积核$$(32\times 8\times 336)$$，步长$$(1,8)$$，两者计算量近似，但前者性能远超后者。
- 在必须减少模型计算量情况下，增大卷积核滑动步长优于增加pooling。
- 在时域滑动$$s>1$$都会影响性能，而卷积核步长$$s=1$$后面接pooling，可以在降采样之前针对相邻帧关系更好的建模，比直接卷积核步长$$s>1$$更有效。

### 2.3.3 RNN
CNN建模的一个缺陷是，一般尺寸的卷积核不足以表达整个唤醒词上下文，而RNN正好擅长基于上下文建模。RNN的缺点在于学不到连续频谱的空间关系，而CNN正好擅长基于空间关系建模。因此ASR任务中出现将CNN和RNN结合的CRNN模型结构，并以CTC作为loss函数，baidu将这个模型结构应用在唤醒任务上，并大幅缩减了模型参数量。








