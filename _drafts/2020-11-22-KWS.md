

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                    tex2jax: {
                    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
                    inlineMath: [['$','$']]
                    }
                });
    </script>
</head>




# Accurate Detection of Wake Word Start and End Using a CNN
> amazon
> Proceedings of INTERSPEECH 2020
> https://arxiv.org/abs/2008.03790

## 1. Introduction
两种方法来检测唤醒词的自边界。
1. 在kws cnn的中间表达基础上，训练第二个回归模型，用于在输入window内预测kw边界。
2. multi-aligned CNN模型，用于在不同alignment上检测kw


## 2. Word-level Keyword Spotting model
![endpoint-f1](/Users/shuo/data/blog/assets/kws/endpoint-f1.jpeg)

模型学习产生，在给定输入语音段内找到唤醒词的后验概率，输出二分类。
- 输入64位log mel谱，$[76,64]$
- 卷积#1 kernel $[5,5]$，channel 96，输出$[72,60,96]$
- max pooling $[2,3]$，输出$[36,20,96]$
- 卷积#2 kernel $[7,3]$，channel 192，stride $[3,1]$，输出$[10,18,192]$
- max pooling $[1,2]$，输出$[10,9,192]$
- 卷积#3 kernel $[4,3]$，channel 192，输出$[7,7,192]$
- 卷积#4 kernel $[3,3]$，channel 192，输出$[5,5,192]$
- 卷积#4 kernel $[3,3]$，channel 192，输出$[3,3,192]$
- FC#1
- FC#2
- FC#3


## 3. Baseline Methods for WW endpoints detection
通常唤醒算法在训练时候的目标是，在给定输入窗长内，均匀地检测可变长度的唤醒词，那么意味着在没有唤醒词长度信息的情况下，我们不可能知道唤醒词的起始和结束。
> 但是对于帧级别对齐比如AM+HMM的唤醒算法，我们可以准确知道唤醒的结束位置，因为训练语料是帧对齐的，起始位置的确不知道，但是很多实际应用中，知道结束位置就够了。

### 3.1. AM+HMM endpointing
声学模型产生后验概率，HMM force-align，天生可以检测到HMM状态中的第一个和最后一个senones时间。

### 3.2. Constant offset endpointing
固定offset方法是检测唤醒词endpoint最简单的方法。
比如90%的alexa发音都在500ms-900ms，然后我们从唤醒事件发生时候开始，适当地选择一个固定偏移，得到起始和结束。

## 4. WW endpoints detection in WL KWS
### 4.1. WW start-end regression model method
这个方法里，在唤醒词检测CNN的同时，增加一个回归模型。
![endpoint-f2](/Users/shuo/data/blog/assets/kws//endpoint-f2.jpeg)

先训练一个不带endpoint预测分支的CNN唤醒模型，再增加一个分支做start-end回归模型，主要思想是，CNN唤醒模型的中间表达可以用来预测唤醒词的endpoint。尝试同时多任务训练唤醒检测和endpoint检测，或者固定CNN唤醒模型，独立训练endpoint检测，发现分开训练更好。
在CNN唤醒模型的两层conv+pooling之后，进入回归模型，经过conv+pooling+fc输出两个值，代表在输入窗内唤醒词start和end的offset，归一化到输入窗长。0表示输入窗长的起始位置，1表示窗长的终止位置。当唤醒词超过窗长时，start值会变成负数，end值会大于1。

### 4.2. Multi-aligned output WW model
![endpoint-f3](/Users/shuo/data/blog/assets/kws/endpoint-f3.jpeg)

训练样本准备，唤醒词出现在一个窗长内不同位置。
- 唤醒事件output，WW center-aligned，当唤醒词中心对齐到输入窗长中心时，触发事件
- end output，WW end-aligned，当唤醒词end边界对齐到输入窗长end边界时，触发事件
- start output，post-center-aligned，当唤醒词中心边界对齐到输入窗长end边界时，触发事件，并不与唤醒词start边界对齐，而是稍微延后一些，是为了减少start检测的latency。

![endpoint-f4](/Users/shuo/data/blog/assets/kws//endpoint-f4.jpeg)

训练中不同alignment的example混合在minibatch内，每个minibatch内分布是：center/start/end/negative = 25%:12.5%:12.5%:50%，这种分布性能最好。center类型的example更多是为了保证更好的检测性能。在训练过程中动态生成minibatch，随机选择alignment。

## 5. Experiments and results
alexa唤醒词，12M正样本+5M反样本，远场数据。无论回归还是multi-aligned方案，唤醒词的endpoint label都是必须的。手动标注唤醒词endpoint非常困难费劲，因此我们用AM+HMM kws方法产生pseudo-ground truth。基于AM+HMM生成大量的唤醒词居中对齐的2s语音片段。
交叉熵loss + tensorflow + 4k minibatch，回归模型基于50k steps的唤醒CNN模型。
测试集中包含大部分AM+HMM标注数据集和一小部分人工标注数据集。

- cnn align表示multi-aligned模型
- cnn regression multi task表示CNN唤醒+回归的多任务同时训练的模型
- thres crossing/local max表示固化CNN唤醒模型，只训练回归模型，回归算法不同
	- local max，当原始唤醒词的后延概率达到最大时，用此时的start-end输出来计算
	- thres crossing，平滑之后后验概率大于阈值时，用此时的start-end输出来计算，目的是降低时延
endpoint检测准确性的衡量指标是start和end误差的标准差。
统计的时候区分了唤醒词长度，因为长唤醒词的检测精度可能变化很大。

结论
- 两种方法在检测唤醒词start准确率接近，而cnn align方法在检测唤醒词end更优。
- 总体准确率高的算法比如local max，在长唤醒词场景反而更差。
- 在人工标注测试集上测试，cnn align/regression两种方法都达到AM+HMM准确率，误差std在52ms，而二次人工标注的误差std在30ms






